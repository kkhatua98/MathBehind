{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d44d29b7",
   "metadata": {},
   "source": [
    "# Mathematics behind Logistic Regression (Binomial)\n",
    "\n",
    "> Learn how Logistic Regression is different from Linear Regression; formulate the problem, dervie the Hessian matrix and show that the problem is negative definite.\n",
    "\n",
    "- toc:true\n",
    "- badges:true\n",
    "- comments:true\n",
    "- categories:[Logistic Regression, MLE, Hessian Matrix]\n",
    "- image: images/Logistic_Regression.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8e632e",
   "metadata": {},
   "source": [
    "Let $\\vec{x} = (x_1, x_2, x_3, ..., x_p)^T$ is the p dimensional independent vector and $y$ is the categorical dependent variable. For now suppose $y$ has only two categories (for more than two categories mathematics is little different). We will always represent categories by 0 and 1. In some cases to do that we may have to use appropriate technic to represent categories by 0 and 1. For example, if categories are boy or girl then we can represent 0 for boy and 1 for girl, alternatively 0 for girl and 1 for boy.\n",
    "\n",
    "## Mathematical Representation\n",
    "Now,we may try to fit a linear regression model of the form,\n",
    "$y_i=\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\beta_3 x_{i3} + ... + \\beta_p x_{ip} + \\epsilon_i$\n",
    "then, as all $y$s are not same, mix of 0 and 1, all the coefficient estimates can not be zero at the same time; ie. the regression plane would have non-zero slope along some axis. So, for suitable choice of $x$ vector predicted value of y would be greater than 0, similarly for suitable choice of x predicted value of y can be negative (becomes unbounded briefly said). So, what we do is, we set,\n",
    "$$y_i \\sim Bernoulli(p_i)$$,\n",
    "where, $$p_i = P(y_i = 1) = \\frac{1}{1+ exp[-(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\beta_3 x_{i3} + ... + \\beta_p x_{ip})]}$$\n",
    "and estimate the parameters by **MLE**.\n",
    "\n",
    "## Coefficient Estimates by MLE\n",
    "### Form of the Log Likelihood\n",
    "Likelihood,\n",
    "$$\\begin{aligned}\n",
    "L &= \\prod_{i=1}^nP(y_i) \\\\\n",
    "  &= \\prod_{i=1}^np_i^{y_i}(1 - p_i)^{1 - y_i} \n",
    "\\end{aligned}$$\n",
    "\n",
    "So, log-likelihood,\n",
    "$$\\begin{aligned}\n",
    "l &= \\log L \\\\\n",
    "  &= \\log \\prod_{i=1}^np_i^{y_i}(1 - p_i)^{1 - y_i} \\\\\n",
    "  &= \\sum_{i=1}^n y_i\\log p_i + (1-y_i)\\log(1-p_i) \\\\\n",
    "  &= \\sum_{i=1}^n y_i.\\log \\frac{1}{1+ exp[-\\beta^T.\\vec{x}_i]} + (1-y_i).\\log \\frac{exp[-\\beta^T.\\vec{x}_i]}{1+ exp[-\\beta^T.\\vec{x}_i]} \\\\\n",
    "  &= \\sum_{i=1}^n -y_i.\\log (1+ exp[-\\beta^T.\\vec{x}_i]) + (1-y_i).\\log exp[-\\beta^T.\\vec{x}_i] \\ - (1-y_i).\\log (1+ exp[-\\beta^T.\\vec{x}_i])\\\\\n",
    "  &= \\sum_{i=1}^n -y_i.\\log (1+ exp[-\\beta^T.\\vec{x}_i]) - (1-y_i).\\beta^T.\\vec{x}_i \\ - (1-y_i).\\log (1+ exp[-\\beta^T.\\vec{x}_i])\\\\\n",
    "  &= \\sum_{i=1}^n - (1-y_i).\\beta^T.\\vec{x}_i \\ - \\log (1+ exp[-\\beta^T.\\vec{x}_i])\\\\\n",
    "  &= -\\sum_{i=1}^n (1-y_i).\\beta^T.\\vec{x}_i \\ + \\log (1+ exp[-\\beta^T.\\vec{x}_i])\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We will get the solution by solving $\\frac{\\partial l}{\\partial \\beta} = 0$; \n",
    "\n",
    "### Deriving the First Derivative\n",
    "Now,\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial l}{\\partial \\beta_m} &= \\frac{\\partial}{\\partial \\beta_m} -\\sum_{i=1}^n (1-y_i).\\beta^T.\\vec{x}_i \\ + \\log (1+ exp[-\\beta^T.\\vec{x}_i]) \\\\\n",
    " &= -\\sum_{i=1}^n (1-y_i).\\frac{\\partial}{\\partial \\beta_m} \\beta^T.\\vec{x}_i \\ + \\frac{\\partial}{\\partial \\beta_m}\\log (1+ exp[-\\beta^T.\\vec{x}_i]) \\\\\n",
    " &= -\\sum_{i=1}^n (1-y_i).x_{im} \\ + \\frac{1}{1+ exp[-\\beta^T.\\vec{x}_i]}.exp[-\\beta^T.\\vec{x}_i].(-1).x_{im} \\\\\n",
    " &= -\\sum_{i=1}^n (1-y_i).x_{im} \\ + \\sum_{i=1}^n \\frac{exp[-\\beta^T.\\vec{x}_i]}{1+ exp[-\\beta^T.\\vec{x}_i]}.x_{im} \\\\\n",
    " &= -\\sum_{i=1}^n (1-y_i).x_{im} \\ + \\sum_{i=1}^n \\frac{1}{1+ exp[\\beta^T.\\vec{x}_i]}.x_{im}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now, $\\frac{\\partial l}{\\partial \\beta} = 0$ will produce global maximum, if we prove that $l$ is concave function with respect to the model coefficients, ie. the **Hessian Matrix** is negative definite.\n",
    "$$H = \\begin{pmatrix}\\begin{pmatrix}\\frac{\\partial^2 l}{\\partial \\beta_n \\beta_m}\\end{pmatrix}\\end{pmatrix} < 0$$\n",
    "\n",
    "\n",
    "### Deriving the Second Derivative\n",
    "Here,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial^2 l}{\\partial \\beta_n \\partial \\beta_m} &= \\frac{\\partial}{\\partial \\beta_n} \\frac{\\partial l}{\\partial \\beta_m} \\\\\n",
    " &= \\frac{\\partial}{\\partial \\beta_n} \\begin{bmatrix} -\\sum_{i=1}^n (1-y_i).x_{im} \\ + \\sum_{i=1}^n \\frac{1}{1+ exp[\\beta^T.\\vec{x}_i]}.x_{im} \\end{bmatrix} \\\\\n",
    " &= \\frac{\\partial}{\\partial \\beta_n} \\sum_{i=1}^n \\frac{1}{1+ exp[\\beta^T.\\vec{x}_i]}.x_{im} \\\\\n",
    " &= \\sum_{i=1}^n \\frac{-1}{(1+ exp[\\beta^T.\\vec{x}_i])^2}.exp[\\beta^T.\\vec{x}_i].x_{in}.x_{im} \\\\\n",
    " &= -\\sum_{i=1}^n \\frac{1}{1+ exp[\\beta^T.\\vec{x}_i]}.\\frac{exp[\\beta^T.\\vec{x}_i]}{1+ exp[\\beta^T.\\vec{x}_i]}.x_{in}.x_{im} \\\\\n",
    " &= -\\sum_{i=1}^n \\frac{exp[-\\beta^T.\\vec{x}_i]}{1+ exp[-\\beta^T.\\vec{x}_i]}.\\frac{1}{1+ exp[-\\beta^T.\\vec{x}_i]}.x_{in}.x_{im} \\\\\n",
    " &= -\\sum_{i=1}^n P(x_i = 1).P(x_i = 0).x_{in}.x_{im} \\\\\n",
    " &= -\\sum_{i=1}^n p_i.(1-p_i).x_{in}.x_{im}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So,\n",
    "$$\\begin{aligned}\n",
    "((\\frac{\\partial^2 l}{\\partial \\beta_n \\partial \\beta_m}))_{ij} &= -\\sum_{i=1}^n p_i.(1-p_i).x_{in}.x_{im}\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "### Forming the Hessian Matrix and Proving it is Negative Definite\n",
    "Now consider a row, say $s$th row $(n = s)$; it is of the form,\n",
    "$$\n",
    "\\begin{aligned}\n",
    " &\\begin{pmatrix}-\\sum_{i=1}^n p_i.(1-p_i).x_{is}.x_{i1} \\ \\ -\\sum_{i=1}^n p_i.(1-p_i).x_{is}.x_{i2} \\ \\ . . . \\ \\ -\\sum_{i=1}^n p_i.(1-p_i).x_{is}.x_{ip}\\end{pmatrix} \\\\\n",
    " &= -\\sum_{i=1}^n p_i.(1-p_i)\\begin{pmatrix}x_{is}.x_{i1} \\ \\ \\ x_{is}.x_{i2} \\ \\ \\ ... \\ \\ \\ x_{is}.x_{ip}\\end{pmatrix} \\\\\n",
    " &= -\\sum_{i=1}^n p_i.(1-p_i).x_{is}\\begin{pmatrix}x_{i1} \\ \\ \\ x_{i2} \\ \\ \\ ... \\ \\ \\ x_{ip}\\end{pmatrix}\n",
    "\\end{aligned}$$\n",
    "\n",
    "So, we will get the Hessian matrix if we stack p such rows one on top of the other like this:\n",
    "$$\\begin{aligned}\n",
    " H &= \\begin{pmatrix}\n",
    " -\\sum_{i=1}^n p_i.(1-p_i).x_{i1}\\begin{pmatrix}x_{i1} \\ \\ \\ x_{i2} \\ \\ \\ ... \\ \\ \\ x_{ip}\\end{pmatrix} \\\\\n",
    " -\\sum_{i=1}^n p_i.(1-p_i).x_{i2}\\begin{pmatrix}x_{i1} \\ \\ \\ x_{i2} \\ \\ \\ ... \\ \\ \\ x_{ip}\\end{pmatrix} \\\\\n",
    " . \\\\\n",
    " . \\\\\n",
    " . \\\\\n",
    " -\\sum_{i=1}^n p_i.(1-p_i).x_{ip}\\begin{pmatrix}x_{i1} \\ \\ \\ x_{i2} \\ \\ \\ ... \\ \\ \\ x_{ip}\\end{pmatrix}\n",
    " \\end{pmatrix} \\\\\n",
    " &= -\\sum_{i=1}^n p_i.(1-p_i)\\begin{pmatrix}\n",
    " x_{i1}\\begin{pmatrix}x_{i1} \\ \\ \\ x_{i2} \\ \\ \\ ... \\ \\ \\ x_{ip}\\end{pmatrix} \\\\\n",
    " x_{i2}\\begin{pmatrix}x_{i1} \\ \\ \\ x_{i2} \\ \\ \\ ... \\ \\ \\ x_{ip}\\end{pmatrix} \\\\\n",
    " . \\\\\n",
    " . \\\\\n",
    " . \\\\\n",
    " x_{ip}\\begin{pmatrix}x_{i1} \\ \\ \\ x_{i2} \\ \\ \\ ... \\ \\ \\ x_{ip}\\end{pmatrix}\n",
    " \\end{pmatrix} \\\\\n",
    " &= -\\sum_{i=1}^n p_i.(1-p_i). \\vec{x}_i .\\vec{x}_i^T \\\\\n",
    " &= -\\sum_{i=1}^n \\sqrt{p_i.(1-p_i)}.\\vec{x}_i.\\sqrt{p_i.(1-p_i)}.\\vec{x}_i^T \\\\\n",
    " &= -\\sum_{i=1}^n \\vec{x^*}_i .\\vec{x^*}_i^T \\ \\ \\leq \\ \\ 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "Here, $\\vec{x^*}_i = \\sqrt{p_i.(1-p_i)}.\\vec{x}_i$. The last equality holds for the fact that $\\vec{x^*}_i .\\vec{x^*}_i^T$ is positive definite and sum of positive definite metrices is positive definite.\n",
    "\n",
    "Now if we set $\\frac{\\partial l}{\\partial \\beta_m} = 0$, we cannot get explicit form of $\\beta$ due the complicated form of $\\frac{\\partial l}{\\partial \\beta_m}$, so to get the solution we have to use numerical methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403ca006",
   "metadata": {},
   "source": [
    "Link to [Notebook 2](_notebooks/2021-10-17-PCA.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
